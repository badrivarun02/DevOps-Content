## **Understanding Kubernetes Ingress** 🚪

- Before the release of Kubernetes version 1.1 on December 1, 2015, Ingress was not available 🤔. People used Kubernetes with just the service concept, without Ingress. 
They created a deployment, which created a pod and provided auto-healing and auto-scaling features 💪. A service was then created on top of it to expose the application within or outside the Kubernetes cluster using the load balancer mode of the service.

- However, some practical problems were realized after using Kubernetes 😕. Users who migrated to Kubernetes from legacy systems like virtual machines or physical servers used to install their applications on top of them and use a load balancer like NGINX or other enterprise load balancers. **Kubernetes does offer a simple round-robin load balancing mechanism** 🔄 with this features provided by creating a deployment and a service. However, they later realized that the load balancing mechanism provided by the service was a simple round-robin load balancing 😞. This means that if you are doing 10 requests, the service using Kube proxy updates your IP table rules and sends 5 requests to pod number one and 5 requests to pod number two, assuming there are two pods 📊. This is a very simple load balancing mechanism compared to the features offered by enterprise load balancers used in virtual machines 💻, which made people unhappy with Kubernetes 😔. This is problem number one 🔍.
     - NOTE: **Round-robin load balancing** 🔄 is a simple algorithm that distributes incoming requests evenly across a group of servers 💻.While it can work well for small and simple projects 🌱, it may not be sufficient for more advanced and complex projects 🏭 that require more sophisticated load balancing capabilities. For example, Enterprise Load Balancers used in virtual machines offer hundreds of features like web application firewall 🔥, TLS security 🔒, ratio-based, sticky sessions 🍯, path-based, domain or host-based load balancing 🛣️, whitelisting ✅, and blacklisting ❌. These advanced features may be necessary for larger and more complex projects to ensure optimal performance and security 🔍.
       Ultimately, the suitability of round-robin load balancing depends on the specific requirements of the project 📋.

- Problem number two is that you can expose your applications to the external world using the load balancer mode of the service, but every time you create a service as a type load balancer mode, the cloud provider charges you for each static public load balancing IP address 💸. If there are thousands of microservices or services required for your applications on Kubernetes, the cloud provider charges heavily 💰. In contrast, on physical or virtual servers, people used to create one load balancer for multiple applications and only expose the load balancer with a static public IP address. In Kubernetes, you are exposing thousands of IP addresses and getting charged 💳. This is problem number two.

### **Summarry:**
 - The first problem is that enterprise and TLS secure load balancing capabilities are missing in Kubernetes 🔍. People who migrated from virtual machines had very good load balancing capabilities like sticky sessions and HTTPS-based load balancing 🔐, which are missing in Kubernetes.

 - Kubernetes is also missing path-based, host-based, and domain-based load balancing 🛣️. For example, if a request is going to amazon.com 🇺🇸 , it should go to a specific application, and if it’s going to amazon.in 🇮🇳 , it should go to another application. There are many other things like ratio-based load balancing that Kubernetes services do not offer.

 - The second problem is that if you are creating a service of type load balancer, the cloud provider will charge you for each service 💰. This is an important interview question ❓. The difference between a load balancer type service and traditional Kubernetes Ingress is that the load balancer type service was good but was missing all of these capabilities 🤔. Also, the cloud provider will charge you for each load balancer service type 💸. If there are thousands of services, you will be charged for thousands of static public IP addresses for load balancers.

### **Solution's:**
 - Ingress was created to solve these problems with load balancer service types in Kubernetes 🚪 . Kubernetes admitted the problem and implemented Ingress, allowing users to create an Ingress resource 📝 . Companies like NGINX, F5, Ambassador, and HAProxy 🔗 , which were top load balancers used on virtual machines 🖥️ , were told to create an Ingress controller 👨‍💻 . As a Kubernetes user 👤 , you can create an Ingress resource and specify the type of routing you want 🛣️ , such as path-based routing. The Ingress controller 👨‍💻 , created by the load balancer company 🔗 , will watch for the Ingress resource 📝 and provide the specified routing 🛣️ . For example, if you want to use NGINX as your load balancer 🔗 , the NGINX company will write an NGINX Ingress controller 👨‍💻 . As a Kubernetes user 👤 , you can deploy the Ingress controller using Helm charts or YAML manifests ⛵ . Then, the developer or DevOps engineer 👩‍💻 will create the Ingress YAML resource for their Kubernetes services 📝 . The Ingress controller 👨‍💻 will watch for the Ingress resource 📝 and provide the path-based routing 🛣️ .

- In conclusion 🎓 , Ingress was introduced in Kubernetes to solve two problems: the lack of enterprise-level load balancing capabilities 🔍 and the charges incurred by cloud providers for each static public load balancing IP address when using a service of type load balancer mode 💸 . By understanding these problems and how Ingress solves them 🧐 , we can better appreciate the value of this powerful tool in our Kubernetes deployments 💪.
