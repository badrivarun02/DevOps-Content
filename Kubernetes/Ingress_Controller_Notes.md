## **Understanding Kubernetes Ingress** ğŸšª

- Before the release of Kubernetes version 1.1 on December 1, 2015, Ingress was not available ğŸ¤”. People used Kubernetes with just the service concept, without Ingress. 
They created a deployment, which created a pod and provided auto-healing and auto-scaling features ğŸ’ª. A service was then created on top of it to expose the application within or outside the Kubernetes cluster using the load balancer mode of the service.

- However, some practical problems were realized after using Kubernetes ğŸ˜•. Users who migrated to Kubernetes from legacy systems like virtual machines or physical servers used to install their applications on top of them and use a load balancer like NGINX or other enterprise load balancers. **Kubernetes does offer a simple round-robin load balancing mechanism** ğŸ”„ with this features provided by creating a deployment and a service. However, they later realized that the load balancing mechanism provided by the service was a simple round-robin load balancing ğŸ˜. This means that if you are doing 10 requests, the service using Kube proxy updates your IP table rules and sends 5 requests to pod number one and 5 requests to pod number two, assuming there are two pods ğŸ“Š. This is a very simple load balancing mechanism compared to the features offered by enterprise load balancers used in virtual machines ğŸ’», which made people unhappy with Kubernetes ğŸ˜”. This is problem number one ğŸ”.
     - NOTE: **Round-robin load balancing** ğŸ”„ is a simple algorithm that distributes incoming requests evenly across a group of servers ğŸ’».While it can work well for small and simple projects ğŸŒ±, it may not be sufficient for more advanced and complex projects ğŸ­ that require more sophisticated load balancing capabilities. For example, Enterprise Load Balancers used in virtual machines offer hundreds of features like web application firewall ğŸ”¥, TLS security ğŸ”’, ratio-based, sticky sessions ğŸ¯, path-based, domain or host-based load balancing ğŸ›£ï¸, whitelisting âœ…, and blacklisting âŒ. These advanced features may be necessary for larger and more complex projects to ensure optimal performance and security ğŸ”.
       Ultimately, the suitability of round-robin load balancing depends on the specific requirements of the project ğŸ“‹.

- Problem number two is that you can expose your applications to the external world using the load balancer mode of the service, but every time you create a service as a type load balancer mode, the cloud provider charges you for each static public load balancing IP address ğŸ’¸. If there are thousands of microservices or services required for your applications on Kubernetes, the cloud provider charges heavily ğŸ’°. In contrast, on physical or virtual servers, people used to create one load balancer for multiple applications and only expose the load balancer with a static public IP address. In Kubernetes, you are exposing thousands of IP addresses and getting charged ğŸ’³. This is problem number two.

### **Summarry:**
 - The first problem is that enterprise and TLS secure load balancing capabilities are missing in Kubernetes ğŸ”. People who migrated from virtual machines had very good load balancing capabilities like sticky sessions and HTTPS-based load balancing ğŸ”, which are missing in Kubernetes.

 - Kubernetes is also missing path-based, host-based, and domain-based load balancing ğŸ›£ï¸. For example, if a request is going to amazon.com ğŸ‡ºğŸ‡¸ , it should go to a specific application, and if itâ€™s going to amazon.in ğŸ‡®ğŸ‡³ , it should go to another application. There are many other things like ratio-based load balancing that Kubernetes services do not offer.

 - The second problem is that if you are creating a service of type load balancer, the cloud provider will charge you for each service ğŸ’°. This is an important interview question â“. The difference between a load balancer type service and traditional Kubernetes Ingress is that the load balancer type service was good but was missing all of these capabilities ğŸ¤”. Also, the cloud provider will charge you for each load balancer service type ğŸ’¸. If there are thousands of services, you will be charged for thousands of static public IP addresses for load balancers.

### **Solution's:**
 - Ingress was created to solve these problems with load balancer service types in Kubernetes ğŸšª . Kubernetes admitted the problem and implemented Ingress, allowing users to create an Ingress resource ğŸ“ . Companies like NGINX, F5, Ambassador, and HAProxy ğŸ”— , which were top load balancers used on virtual machines ğŸ–¥ï¸ , were told to create an Ingress controller ğŸ‘¨â€ğŸ’» . As a Kubernetes user ğŸ‘¤ , you can create an Ingress resource and specify the type of routing you want ğŸ›£ï¸ , such as path-based routing. The Ingress controller ğŸ‘¨â€ğŸ’» , created by the load balancer company ğŸ”— , will watch for the Ingress resource ğŸ“ and provide the specified routing ğŸ›£ï¸ . For example, if you want to use NGINX as your load balancer ğŸ”— , the NGINX company will write an NGINX Ingress controller ğŸ‘¨â€ğŸ’» . As a Kubernetes user ğŸ‘¤ , you can deploy the Ingress controller using Helm charts or YAML manifests â›µ . Then, the developer or DevOps engineer ğŸ‘©â€ğŸ’» will create the Ingress YAML resource for their Kubernetes services ğŸ“ . The Ingress controller ğŸ‘¨â€ğŸ’» will watch for the Ingress resource ğŸ“ and provide the path-based routing ğŸ›£ï¸ .

- In conclusion ğŸ“ , Ingress was introduced in Kubernetes to solve two problems: the lack of enterprise-level load balancing capabilities ğŸ” and the charges incurred by cloud providers for each static public load balancing IP address when using a service of type load balancer mode ğŸ’¸ . By understanding these problems and how Ingress solves them ğŸ§ , we can better appreciate the value of this powerful tool in our Kubernetes deployments ğŸ’ª.
